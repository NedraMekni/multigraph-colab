{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install missing libraries"
      ],
      "metadata": {
        "id": "xYAi5BWRvkCP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k2ong5_vDNk",
        "outputId": "87c4dfcd-f857-4b24-b071-7c9c67ec5172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:MDAnalysis.coordinates.AMBER:netCDF4 is not available. Writing AMBER ncdf files will be slow.\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import csv\n",
        "import MDAnalysis as mda\n",
        "\n",
        "from Bio.PDB import *\n",
        "from torch_geometric.data import Data\n",
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "parser = PDBParser()\n",
        "device = \"cpu\"  # torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "CACHE_PDB = None\n",
        "\n",
        "\n",
        "def get_cache(cache_fname: str, cached_graph: str) -> None:\n",
        "\n",
        "    \"\"\"\n",
        "    Get the cached PDB files from a cache file or create a new cache.\n",
        "\n",
        "    Parameters:\n",
        "    - cache_fname (str): A string specifying the cache file name.\n",
        "    - cached_graph (str): A string specifying the directory for the cached PDB files.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    global CACHE_PDB\n",
        "    if os.path.isfile(cache_fname):\n",
        "        if not os.path.exists(cached_graph):\n",
        "            os.mkdir(cached_graph)\n",
        "        with open(cache_fname, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "            CACHE_PDB = [x.strip() for x in lines]\n",
        "\n",
        "    else:\n",
        "        if os.path.exists(cached_graph):\n",
        "            os.system(\"rm -r \" + cached_graph)\n",
        "        os.mkdir(cached_graph)\n",
        "        CACHE_PDB = []\n",
        "\n",
        "    print(\"CACHE PDB = {}\".format(CACHE_PDB))\n",
        "\n",
        "\n",
        "# max_t and min_t are nano\n",
        "def preprocess_csv(fname: str, outfname: str, max_t=100000, min_t=10) -> None:\n",
        "    \"\"\"\n",
        "    Preprocess a CSV file by filtering rows based on activity values and saving the resulting dataframe to a new file.\n",
        "\n",
        "    Parameters:\n",
        "    - fname (str): A string specifying the input CSV file name.\n",
        "    - outfname (str): A string specifying the output CSV file name.\n",
        "    - max_t (int, optional): An integer specifying the maximum activity value to include in the output dataframe. Default is 100000nM.\n",
        "    - min_t (int, optional): An integer specifying the minimum activity value to include in the output dataframe. Default is 10nM.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(fname, usecols=[\" pdb_code\", \" activity\"])\n",
        "    df = df.mask(df.eq(\" None\")).dropna()\n",
        "    df = df.astype({\" activity\": \"float64\"})\n",
        "    df = df[df[\" activity\"].between(min_t, max_t)]\n",
        "    assert max(df[\" activity\"]) <= max_t and min(df[\" activity\"]) >= min_t\n",
        "    new_df_cap = df.copy()\n",
        "    new_df_cap.to_csv(outfname, index=False)\n",
        "\n",
        "\n",
        "def valid_pdb(dirname: str) -> None:\n",
        "    my_files = list(os.walk(dirname))[0]\n",
        "\n",
        "    for z in my_files[2]:\n",
        "        if \"_H.pdb\" not in z:\n",
        "            continue\n",
        "        with open(dirname + \"/\" + z, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "            if (\n",
        "                not any(\n",
        "                    [row.split()[0] == \"ATOM\" for row in lines if len(row.split()) > 0]\n",
        "                )\n",
        "            ) or (\n",
        "                not any(\n",
        "                    [\n",
        "                        row.split()[0] == \"HETATM\"\n",
        "                        for row in lines\n",
        "                        if len(row.split()) > 0\n",
        "                    ]\n",
        "                )\n",
        "            ):\n",
        "                print(\"removing file {}\".format(z))\n",
        "                os.remove(dirname + \"/\" + z)\n",
        "\n",
        "\n",
        "def biograph_from_file(dirname: str) -> dict:\n",
        "    \"\"\"\n",
        "    Generate graph dict from PDB files\n",
        "    graph dict:\n",
        "    -key: PDB file name -> str\n",
        "    -value: graph -> tuple of dicts\n",
        "    \"\"\"\n",
        "    global CACHE_PDB\n",
        "    graph_dict = {}\n",
        "    my_files = list(os.walk(dirname))[0]\n",
        "    print(\"my files before\", my_files[2])\n",
        "    my_files = [x for x in my_files[2] if x not in CACHE_PDB]\n",
        "    print(\"my files \", my_files)\n",
        "    if len(my_files) == 0:\n",
        "        print(\"NO NEW PDB FOUND\")\n",
        "        exit(0)\n",
        "    cntPdb = 0\n",
        "    for z in my_files:\n",
        "        nodes_p_entry = {}\n",
        "        nodes_p = {}\n",
        "        nodes_i = {}\n",
        "        elements_p = {}\n",
        "        if \"_H.pdb\" not in z:\n",
        "            continue\n",
        "        structure = parser.get_structure(z, dirname + \"/\" + z)\n",
        "        print(\"building \", z)\n",
        "        i = 0\n",
        "        for atom in structure.get_atoms():\n",
        "            coord = atom.coord\n",
        "            nodes_p[atom.serial_number] = atom\n",
        "            nodes_i[atom.serial_number] = i\n",
        "            elements_p[atom.serial_number] = atom.element\n",
        "            # check if atom is hetatm -> https://stackoverflow.com/questions/25718201/remove-heteroatoms-from-pdb\n",
        "            tags = atom.get_full_id()\n",
        "            nodes_p_entry[atom.serial_number] = (\n",
        "                \"HETATM\" if tags[3][0] != \" \" else \"ATOM\"\n",
        "            )\n",
        "            i += 1\n",
        "\n",
        "        try:  # for hydro\n",
        "            u = mda.Universe(dirname + \"/\" + z)\n",
        "            if not hasattr(u, \"atoms\") or not all(\n",
        "                [hasattr(atom, \"bonds\") for atom in u.atoms]\n",
        "            ):\n",
        "                # clean_fd_mda(dirname + \"/\" + z)\n",
        "                continue\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        graph_dict[z] = (nodes_p, nodes_p_entry, u.atoms, elements_p, nodes_i)\n",
        "        cntPdb += 1\n",
        "\n",
        "    print(\"Tot pdb = {}\".format(cntPdb))\n",
        "    return graph_dict, my_files\n",
        "\n",
        "\n",
        "def data_activities_from_file(dirname, fname: str, log_transform = True) -> dict:\n",
        "    \"\"\"\n",
        "    Extract data and activities from a file.\n",
        "\n",
        "    Parameters:\n",
        "    - dirname (str): A string specifying the directory name.\n",
        "    - fname (str): A string specifying the file name.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing data and activities extracted from the file.\n",
        "    \"\"\"\n",
        "\n",
        "    label = pd.read_csv(fname, usecols=[\" pdb_code\", \" activity\"],dtype=str)\n",
        "    label.to_csv(dirname + \"/test_y.csv\", index=False)\n",
        "    all_rows = []\n",
        "    y_dict = {}\n",
        "    with open(dirname + \"/test_y.csv\", \"r\") as f:\n",
        "        lines = f.readlines()[1:]\n",
        "\n",
        "        for l in lines:\n",
        "            elements = l.split(\",\")\n",
        "            elements[0] = elements[0].strip()\n",
        "            elements[1] = elements[1].strip()\n",
        "\n",
        "            for k in elements[0].split(\" \"):\n",
        "\n",
        "                if k not in y_dict.keys():\n",
        "                    y_dict[k] = []\n",
        "\n",
        "                try:\n",
        "                    v = float(elements[1])\n",
        "                    if log_transform:\n",
        "                        v = v * (10**-9)\n",
        "                        v = -math.log10(v)\n",
        "                    y_dict[k].append(v)\n",
        "\n",
        "                except:\n",
        "                    # print(\"### cast error, skip activity reading\")\n",
        "                    continue\n",
        "\n",
        "    return y_dict\n",
        "\n",
        "\n",
        "def filter_with_y(graph_dict: dict, y_dict: dict, pre=\"_1\") -> dict:\n",
        "    return {\n",
        "        k: graph_dict[k]\n",
        "        for k in graph_dict.keys()\n",
        "        if k.split(\"_\")[0] + pre in y_dict.keys()\n",
        "    }\n",
        "\n",
        "\n",
        "def get_ohe_order(fname: str) -> list:\n",
        "    \"\"\"\n",
        "    Get the order of the one-hot encoded features from a file.\n",
        "\n",
        "    Parameters:\n",
        "    - fname (str): A string specifying the file name.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of strings containing the one-hot encoded feature names in the order they appear in the file.\n",
        "    \"\"\"\n",
        "    with open(fname, \"r\") as f:\n",
        "        return [x.strip() for x in f.readlines()]\n",
        "\n",
        "\n",
        "def update_old_dict(\n",
        "    diff_ohe_atom_len: int, diff_ohe_element_len: int, graph_dir: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Update the attributes of one-hot encoded atoms and elements in a dictionary of graphs stored in a directory.\n",
        "\n",
        "    Parameters:\n",
        "    - diff_ohe_atom_len (int): An integer specifying the number of additional one-hot encoded atom attributes to add.\n",
        "    - diff_ohe_element_len (int): An integer specifying the number of additional one-hot encoded element attributes to add.\n",
        "    - graph_dir (str): A string specifying the directory containing the stored graphs.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    files = os.listdir(graph_dir + \"/cached_graph\")\n",
        "    files = [\n",
        "        graph_dir + \"/cached_graph/\" + x for x in files if x[-3:] == \".pt\"\n",
        "    ]  # select all .pt files\n",
        "\n",
        "    for f in files:\n",
        "        print(\"opening {}\".format(f))\n",
        "        global_g = torch.load(f)\n",
        "        print(\"after load {}\".format(f))\n",
        "\n",
        "        for k in global_g.keys():\n",
        "            local_g = global_g[k]\n",
        "            for el in local_g[1].keys():\n",
        "                local_g[1][el][\"attributes\"][1] += [0] * diff_ohe_atom_len\n",
        "                local_g[1][el][\"attributes\"][2] += [0] * diff_ohe_element_len\n",
        "        print(\"removing {}\".format(f))\n",
        "\n",
        "        os.remove(f)  # maybe it does not overwrite on save\n",
        "        print(\"saving {}\".format(f))\n",
        "        torch.save(global_g, f)\n",
        "\n",
        "\n",
        "def store_element_cache(fname: str, cache_list: list) -> None:\n",
        "    with open(fname, \"w\") as f:  # no append\n",
        "        for el in cache_list:\n",
        "            f.write(el + \"\\n\")\n",
        "\n",
        "\n",
        "def build_graph_dict(graph_dict, y_dict: dict, ohe_path: str, pre = \"_1\") -> dict:\n",
        "    \"\"\"\n",
        "    Build a graph dictionary from a y dictionary and a path to an OHE file.\n",
        "\n",
        "    Parameters:\n",
        "    - graph_dict (dict): A dictionary containing graph information.\n",
        "    - y_dict (dict): A dictionary containing y values.\n",
        "    - ohe_path (str): A string specifying the path to the OHE file.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing updated graph information.\n",
        "    \"\"\"\n",
        "    global_G = {}\n",
        "    graph_x = {}\n",
        "    print(\"IN BUILD GRAPH\")\n",
        "\n",
        "    # helper structures for one hot encoding labels and for cast dictionary graph into list\n",
        "    atom_type_list = [\n",
        "        [graph_dict[k][0][k1].name for k1 in graph_dict[k][0].keys()]\n",
        "        for k in graph_dict.keys()\n",
        "    ]\n",
        "\n",
        "    atom_type_list = list(set(reduce(lambda x, y: x + y, atom_type_list)))\n",
        "\n",
        "    element_list = [\n",
        "        [graph_dict[k][3][k1] for k1 in graph_dict[k][3].keys()]\n",
        "        for k in graph_dict.keys()\n",
        "    ]\n",
        "    element_list = list(set(reduce(lambda x, y: x + y, element_list)))\n",
        "\n",
        "    # retrieve the order\n",
        "    # get the new order\n",
        "    # merge\n",
        "    # appending a series of 0s to the end of the one-hot encoding\n",
        "    # set the new order to Element_list and atom_type_list\n",
        "\n",
        "    print(\"len cache pdb \", len(CACHE_PDB))\n",
        "\n",
        "    if len(CACHE_PDB) > 0:\n",
        "        print(\"before read get_ohe_order\")\n",
        "        print(\"reading \", ohe_path + \"/.ohe_atom_order\")\n",
        "        old_atom_order = get_ohe_order(ohe_path + \"/.ohe_atom_order\")\n",
        "        old_element_order = get_ohe_order(ohe_path + \"/.ohe_element_order\")\n",
        "        print(\"after read get_ohe_order\")\n",
        "        diff_ohe_atom = [x for x in atom_type_list if x not in old_atom_order]\n",
        "        diff_ohe_element = [x for x in element_list if x not in old_element_order]\n",
        "        if len(diff_ohe_atom) == 0:\n",
        "            atom_type_list = old_atom_order.copy()\n",
        "            element_list = old_element_order.copy()\n",
        "        else:\n",
        "            # update_old_dict\n",
        "            update_old_dict(len(diff_ohe_atom), len(diff_ohe_element), ohe_path)\n",
        "            atom_type_list = old_atom_order.copy()\n",
        "            atom_type_list += diff_ohe_atom\n",
        "\n",
        "            element_list = old_element_order.copy()\n",
        "            element_list += diff_ohe_element\n",
        "\n",
        "    print(\"after ohe read\")\n",
        "    global_node_list_order = {\n",
        "        k: list(graph_dict[k][0].keys()) for k in graph_dict.keys()\n",
        "    }\n",
        "\n",
        "    print(\"building adjacence list for each graph\")\n",
        "    graph_dict = {\n",
        "        k: graph_dict[k]\n",
        "        for k in graph_dict.keys()\n",
        "        if k.split(\"_\")[0] + pre  in y_dict.keys()\n",
        "    }\n",
        "    for fname in graph_dict.keys():\n",
        "        print(\"building adjacence list of \", fname)\n",
        "        my_edges = []\n",
        "        # node_p and node_p_etry are two dictionaries storing nodes\n",
        "\n",
        "        nodes_p = graph_dict[fname][0]\n",
        "        nodes_p_entry = graph_dict[fname][1]\n",
        "        atom_p = graph_dict[fname][2]\n",
        "        elements_p = graph_dict[fname][3]\n",
        "        nodes_i = graph_dict[fname][4]\n",
        "\n",
        "        ns = NeighborSearch(list(nodes_p.values()))\n",
        "        node_list_order = global_node_list_order[fname]  # to build adj list\n",
        "\n",
        "        # bulding H label for each node\n",
        "        # for each atom in atom struct extract the n of H\n",
        "        hydrogen_label = {}\n",
        "        valid_h_pdb = True\n",
        "        for atom in atom_p:\n",
        "            neighbour_atoms = list(atom.bonds)\n",
        "            n_hydro = len(\n",
        "                [bond[1].element for bond in neighbour_atoms if bond[1].element == \"H\"]\n",
        "            )\n",
        "            n_hydro_hot = [0] * 5\n",
        "            if n_hydro >=5:\n",
        "                valid_h_pdb = False\n",
        "                break\n",
        "            #assert n_hydro < 5\n",
        "            n_hydro_hot[n_hydro] = 1\n",
        "            hydrogen_label[atom.id] = n_hydro_hot\n",
        "        if not valid_h_pdb:\n",
        "            print(fname,\" not valid\")\n",
        "            continue\n",
        "        else:\n",
        "            print(fname, \" valid\")\n",
        "\n",
        "        # Multigraph generation\n",
        "        # Find adjacencies for each node in nodes_p that is HETATM add them to my_edges\n",
        "\n",
        "        for k in nodes_p.keys():\n",
        "            if nodes_p_entry[k] == 'ATOM':\n",
        "                continue\n",
        "            node = nodes_p[k]\n",
        "            adjs = [h for h in ns.search(node.get_coord(), 3, \"A\")]\n",
        "            adjs += [h for h in ns.search(node.get_coord(), 6, \"A\")]\n",
        "            adjs += [h for h in ns.search(node.get_coord(), 9, \"A\")]\n",
        "            my_edges += [\n",
        "                (nodes_i[node.serial_number], nodes_i[adj.serial_number])\n",
        "                for adj in adjs\n",
        "            ]\n",
        "\n",
        "        print(\"end treshold loop \", fname)\n",
        "        # build pytorch data graph\n",
        "        nodes_p_final = list(nodes_p.keys())\n",
        "        nodes_p_tensor = [[x] for x in list(nodes_p.keys())]\n",
        "        graph_data_x = torch.tensor(nodes_p_tensor, dtype=torch.float, device=device)\n",
        "        graph_edge = torch.tensor(my_edges, dtype=torch.long, device=device)\n",
        "        y_tensor = torch.tensor(\n",
        "            [[y_dict[fname.split(\"_\")[0] + pre][0]]], dtype=torch.float, device=device\n",
        "        )\n",
        "\n",
        "        G = Data(\n",
        "            x=graph_data_x,\n",
        "            edge_index=graph_edge.t().contiguous(),\n",
        "            y=y_tensor,\n",
        "            dtype=torch.float,\n",
        "        )\n",
        "        graph_x[fname] = nodes_p_final\n",
        "        labels = {}\n",
        "\n",
        "        # build hot-encoding for each node in G.\n",
        "\n",
        "        for node in nodes_p_final:\n",
        "            label_node = [0] if nodes_p_entry[node] == \"ATOM\" else [1]\n",
        "            atom_type_node = [0] * len(atom_type_list)  # inizialise vector all 0\n",
        "            atom_type_node[\n",
        "                atom_type_list.index(nodes_p[node].name)\n",
        "            ] = 1  # build when atom type== 1\n",
        "            element_node = [0] * len(element_list)\n",
        "            element_node[element_list.index(elements_p[node])] = 1\n",
        "            label = {\n",
        "                \"attributes\": [\n",
        "                    label_node,\n",
        "                    atom_type_node,\n",
        "                    element_node,\n",
        "                    hydrogen_label[node],\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            labels[node] = label\n",
        "\n",
        "        global_G[fname] = (G, labels)\n",
        "        print(\"{} ohe completed\".format(fname))\n",
        "\n",
        "    store_element_cache(ohe_path + \"/.ohe_atom_order\", atom_type_list)\n",
        "    store_element_cache(ohe_path + \"/.ohe_element_order\", element_list)\n",
        "\n",
        "    return global_G\n",
        "\n",
        "\n",
        "def euclidean_distance(p:float, q: float) -> float:\n",
        "    \"\"\"\n",
        "    Compute euclidian distance between pair of coordinates\n",
        "    \"\"\"\n",
        "    return math.sqrt(((p[0] - q[0]) ** 2) + ((p[1] - q[1]) ** 2) + ((p[2] - q[2]) ** 2))\n",
        "\n",
        "\n",
        "def save_graphs(global_G: dict, dirname: str) -> None:\n",
        "    my_files = list(os.walk(dirname))[0][2]\n",
        "    torch.save(global_G, dirname + \"/tensorG_{}.pt\".format(len(my_files)))\n",
        "\n",
        "\n",
        "def write_cache(cache_fname: str, cache_list: list) -> None:\n",
        "    with open(cache_fname, \"a\") as f:\n",
        "        for k in cache_list:\n",
        "            f.writelines(k + \"\\n\")\n"
      ],
      "metadata": {
        "id": "hmcAPR7SwMHb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}